{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real time blink detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article we will learn how to make a real time blink detector application using computer vision down the line we will be also using more libraries and mathematics to build such an application we will be going through complete pipeline and code by code analysis for the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of blink detection application\n",
    "\n",
    "1. **Driver drowsiness detection:** As name suggest this application is very useful in building real world application like detecting whether if driver is sleepy or not while detecting the eye moment or blink.\n",
    "\n",
    "\n",
    "2. **Iris tracking:** This is another use case while we can also track the iris movement for building AR kind of application.\n",
    "\n",
    "\n",
    "3. **Virtual gaming:** We are in the age of virtual reality evolution and by far mostly VR powered games are either hand or body movement driven but we can also build games which are eye movement driven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we will gonna achieve this?\n",
    "\n",
    "* Firstly we will extract only those points which are located near our eyes to get the enclosed area of the eye and then we will find out the **EAR (Eye Aspect Ratio)** which will help us to determine that blink as an event has occured or not.\n",
    "* There are total of **6 XY coordinates** for an eye which starts from the left corner of the eye and then from that position it will go to **clockwise direction**.\n",
    "* There will be a relation between the **height and width** of these coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's starts by importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import FileVideoStream\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **distance:** This library will help us to find the **Ecludean distance** and remove some burden of applying mathematical calculations.\n",
    "* **FileVideoStream:** This library will help us to stream the videos from the **file explorer** i.e. video file (.mp4 or other type).\n",
    "* **VideoStream:** This library will help us to stream the **real-time** video from the **webcam**.\n",
    "* **face_utils:** This library will be responsible for grabbing the face landmarks (here eyes).\n",
    "* **numpy:** This library will help us to perform some other **mathematical operations** like arrays.\n",
    "* **time:** This library is to get the system time or to get delayed i.e. **sleep function**.\n",
    "* **dlib:** This is the heart of this application as this library will help us get the access to **68 landmarks** of the face in real-time.\n",
    "* **cv2:** Computer vision library to perform some image processsing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye aspect ratio (EAR) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    \n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "    ear = (A + B)/ (2.0 * C)\n",
    "\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code breakdown:\n",
    "1. First we will get the **Ecludean distance** between the 2 coordinates of eyes\n",
    "2. While grabbing the coordinates we will first have the **vertical eye landmarks**.\n",
    "3. Then we will have the **horizontal eye landmarks** using the same algorithm.\n",
    "4. After grabbing the coordinates we will calculate the **Eye Aspect ratio**.\n",
    "5. Then at the last we will **return the EAR.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Eye aspect ratio constant:** This constant value will act like a threshold value to detect the blink.\n",
    "* **Count of frames:**: This constant value is the threshold value for the number of consecutive frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTER = 0\n",
    "TOTAL = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Counter:** This value will denote the total number of consecutive frames that will have the threshold value less than EYE ASPECT RATIO constant.\n",
    "* **Total:** This value will make a count of total number of blinks in certain number of frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the dlib's face detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dlib's face detector\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the dlib's face detector\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **detector**: Here we will initialize the dlib library (**frontal face detector**).\n",
    "* **predictor**: Now we will use the **shape_predictor** method to load the **.dat** file and predict the landmarks accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the index of facial landmarks (here eye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Firstly we are grabbing the coordinates values of **left eye** using **face_utils** function.\n",
    "* Secondly we will do the same for **right_eye**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the video/ real-time streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the video/live stteaming\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting the video/live stteaming\")\n",
    "vs = FileVideoStream(\"Video.mp4\").start()\n",
    "fileStream = True\n",
    "# vs = VideoStream(src = 0).start() # run this line if you want to run it on webcam.\n",
    "# vs = VideoStream(usePiCamera = True).start()\n",
    "fileStream = False\n",
    "time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code breakdown:\n",
    "\n",
    "1. While using **FileVideoStream** we will initialize the object with the video file location and then **start()** the same.\n",
    "2. Setting the **fileStream** value as **True** after successfull streaming of file (video).\n",
    "3. If we want a **real-time** streaming we will be using the **VideoStream(src=0).start()**.\n",
    "4. Then we will using the **sleep** function from **time** library to delayed the frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    if fileStream and not vs.more():\n",
    "        break\n",
    "\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width = 450)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    for rect in rects:\n",
    "\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "\n",
    "        if ear < EYE_AR_THRESH:\n",
    "            COUNTER += 1\n",
    "\n",
    "        else:\n",
    "            if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                TOTAL += 1\n",
    "            #reset the eye frame counter\n",
    "\n",
    "            COUNTER = 0\n",
    "\n",
    "        cv2.putText(frame, \"Blinks:{}\".format(TOTAL), (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"EAR:{:.2f}\".format(ear), (300, 30), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(12) & 0xFF\n",
    "\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code breakdown:\n",
    "\n",
    "1. Firstly we will loop over the video streaming and along with that we will also check that whether there are any **more frames left in the buffer**.\n",
    "2. Now we will first pick all the **frames** from the video/live streaming then we will **resize** to our desired dimensions and at the last we will convert it to **grayscale**.\n",
    "3. Then using the **detector** function we will detect the faces.\n",
    "4. Now, with the help of **predictor** function we will detect the facial landmarks then convert it into **numpy array**.\n",
    "5. Then we will first grab the **left and right** eye coordinate then will compute the **Eye Aspect Ratio** and do the average by 2 i.e. 2 eyes.\n",
    "6. Then we will compute the **convex hull** for both the eyes so that we can visualise the eyes by **drawing** methods using **contours**.\n",
    "7. Now, we will check that our calculated **EAR** should be below than the **threshold value** so that we can increase the **blink counter**.\n",
    "8. Else if the **EAR** is greater than **threshold value** then we will increase the counter of **Total frames** so that we can check other frames as well also if the eyes were closed for certain frames then also we will increase the **number of blinks**.\n",
    "9. Then using the **putText** methid we will draw the **number of blinks** in each frame and also the **Eye Aspect Ratio (EAR) value**\n",
    "10. Then at the last using the **show** function we will show the main frame and along with that we will also code to exit from the loop i.e. **q** and at the last for clean-up process we will destroy all the windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. Firstly we saw the real world applicaton of **blink detection application** then we saw what we will be doing in a nutshell.\n",
    "2. The main key takeaway from this article is to **segment the eyes** by using it's coordinates.\n",
    "3. We have also learnt about the concept of **ecludean distance** and its formulae using the specific library.\n",
    "4. Along with that we also came across the concept of **Eye Aspect Ratio (EAR)** which is the soul of this application.\n",
    "5. We also learnt how **dlib** library can detect the landmarks of the face and along with that reading the video files as well as live streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video link :\n",
    "https://usercontent.one/wp/www.computervision.zone/wp-content/uploads/2022/01/Video.mp4?media=1632743877"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
